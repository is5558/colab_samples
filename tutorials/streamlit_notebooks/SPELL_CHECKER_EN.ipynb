{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4b_2KemgDWf"
      },
      "source": [
        "\n",
        "\n",
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/SPELL_CHECKER_EN.ipynb)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_HwrduqwTArI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnZG0I4ogNLI"
      },
      "source": [
        "# **Spell check your text documents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apjCmRyjgQll"
      },
      "source": [
        "## 1. Colab Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2phEj9SygX4n"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uAiXj3DOfyZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f216c00f-3317-4c59-b1e6-c43d02b02040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/718.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.5/718.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.8/718.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install PySpark and Spark NLP\n",
        "! pip install -q pyspark spark-nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE2XyDI7NHtg"
      },
      "source": [
        "Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "\n",
        "spark = sparknlp.start()\n",
        "pipeline1 = PretrainedPipeline(\"check_spelling\", lang=\"en\")\n",
        "pipeline2 = ContextSpellCheckerModel.pretrained('spellcheck_dl', lang=\"en\").setInputCols(\"token\").setOutputCol(\"corrected\")\n",
        "\n",
        "\n",
        "text = \"Lets uss fixx the spellling mistakess in this sentense.\"\n",
        "result1 = pipeline1.fullAnnotate(text)\n",
        "# result2 = pipeline2.fullAnnotate(text)\n",
        "for token in result1[0][\"checked\"]:\n",
        "    print(f\"{token.result} ({token.begin}-{token.end})\")\n",
        "# for token in result2[0][\"checked\"]:\n",
        "#     print(f\"{token.result} ({token.begin}-{token.end})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DJnasw_YhUo",
        "outputId": "3315037f-aef0-441f-a56a-d4a9d2930529"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning::Spark Session already created, some configs may not take.\n",
            "check_spelling download started this may take some time.\n",
            "Approx size to download 884.9 KB\n",
            "[OK!]\n",
            "spellcheck_dl download started this may take some time.\n",
            "Approximate size to download 95.1 MB\n",
            "[OK!]\n",
            "Lets (0-3)\n",
            "uss (5-7)\n",
            "fix (9-12)\n",
            "the (14-16)\n",
            "spelling (18-26)\n",
            "mistake (28-36)\n",
            "in (38-39)\n",
            "this (41-44)\n",
            "sent (46-53)\n",
            ". (54-54)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "g9hfxX3df3n3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyspark\n",
        "\n",
        "import sparknlp\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from pyspark.sql.types import StringType, IntegerType\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3Ur62RrgaxX"
      },
      "source": [
        "Start Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "763GC_wVNN6F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6577ff8-4367-4b7b-8744-115b794af31b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning::Spark Session already created, some configs may not take.\n",
            "Spark NLP version 6.0.4\n",
            "Apache Spark version: 3.5.1\n",
            "Spark version: 3.5.1\n"
          ]
        }
      ],
      "source": [
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version\", sparknlp.version())\n",
        "print(\"Apache Spark version:\", spark.version)\n",
        "print(\"Spark version:\", pyspark.__version__)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUP6-XeQgeQW"
      },
      "source": [
        "## 2. Select the NER model and construct the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taOqLG1Ogc3D",
        "outputId": "f67f272e-95c5-4efd-d20b-f017964bfc3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spellcheck_dl download started this may take some time.\n",
            "Approximate size to download 95.1 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "document_assembler = DocumentAssembler()\\\n",
        "  .setInputCol(\"text\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = RecursiveTokenizer()\\\n",
        "  .setInputCols([\"document\"])\\\n",
        "  .setOutputCol(\"token\")\\\n",
        "  .setPrefixes([\"\\\"\", \"(\", \"[\", \"\\n\"])\\\n",
        "  .setSuffixes([\".\", \",\", \"?\", \")\",\"!\", \"‘s\"])\n",
        "\n",
        "spell_model = ContextSpellCheckerModel\\\n",
        "    .pretrained('spellcheck_dl')\\\n",
        "    .setInputCols(\"token\")\\\n",
        "    .setOutputCol(\"corrected\")\n",
        "\n",
        "finisher = Finisher().setInputCols(\"corrected\")\n",
        "\n",
        "light_pipeline = Pipeline(stages = [document_assembler,\n",
        "                                    tokenizer,\n",
        "                                    spell_model,\n",
        "                                    finisher])\n",
        "## For comparison\n",
        "full_pipeline = Pipeline(stages = [document_assembler,\n",
        "                                   tokenizer,\n",
        "                                   spell_model])\n",
        "\n",
        "empty_ds = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "pipeline_model = full_pipeline.fit(empty_ds)\n",
        "l_pipeline_model = LightPipeline(light_pipeline.fit(empty_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_DzGuMPibKr"
      },
      "source": [
        "## 3. Create example inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c7yPd884i_XQ"
      },
      "outputs": [],
      "source": [
        "# Enter examples as strings in this array\n",
        "input_list = [\"Plaese alliow me tao introdduce myhelf, I am a man of waelth und tiaste\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvyOmrgHjO5J"
      },
      "source": [
        "## 4. Use the pipeline to create outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICREynF-jzn8"
      },
      "source": [
        "Full Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RLbrPvC3jOSw"
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame(pd.DataFrame({\"text\": input_list}))\n",
        "result = pipeline_model.transform(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhlbvs1Dj1ck"
      },
      "source": [
        "Light Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YulVrYgAj2ex"
      },
      "outputs": [],
      "source": [
        "# Light pipelines expect a single example.\n",
        "light_result = l_pipeline_model.annotate(input_list[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk72hQGqjX3f"
      },
      "source": [
        "## 5. Visualize results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9JDK9_EjaF_"
      },
      "source": [
        "Visualize comparison as dataframe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select(F.explode(F.arrays_zip(result.token.result,\n",
        "                                     result.corrected.result)).alias(\"cols\")) \\\n",
        "      .select(F.expr(\"cols['0']\").alias(\"original\"),\n",
        "              F.expr(\"cols['1']\").alias('corrected')).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uekWfx_tFL3x",
        "outputId": "61989fd6-c7eb-4b9e-d7b5-1d8cb963fe29"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+\n",
            "|original  |corrected|\n",
            "+----------+---------+\n",
            "|Plaese    |Please   |\n",
            "|alliow    |allow    |\n",
            "|me        |me       |\n",
            "|tao       |to       |\n",
            "|introdduce|introduce|\n",
            "|myhelf    |myself   |\n",
            "|,         |,        |\n",
            "|I         |I        |\n",
            "|am        |am       |\n",
            "|a         |a        |\n",
            "|man       |man      |\n",
            "|of        |of       |\n",
            "|waelth    |wealth   |\n",
            "|und       |and      |\n",
            "|tiaste    |taste    |\n",
            "+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yItQfZmhcji"
      },
      "source": [
        "Vizualise light pipeline and finished result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zHSiB0skP83",
        "outputId": "4d5489df-b98e-43ed-ee8a-9cbd0ad1d4f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Please',\n",
              " 'allow',\n",
              " 'me',\n",
              " 'to',\n",
              " 'introduce',\n",
              " 'myself',\n",
              " ',',\n",
              " 'I',\n",
              " 'am',\n",
              " 'a',\n",
              " 'man',\n",
              " 'of',\n",
              " 'wealth',\n",
              " 'and',\n",
              " 'taste']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# this finished result does not need parsing and can directly be used an any other task.\n",
        "light_result['corrected']"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "45150093197569bb3a58481dcd32cd1adb45462fa3448719e8ac38ada6166aca"
    },
    "kernelspec": {
      "display_name": "Python 3.6.10 64-bit ('tensorflow2_p36': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}