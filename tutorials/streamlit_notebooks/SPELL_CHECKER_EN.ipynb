{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/is5558/colab_samples/blob/main/tutorials/streamlit_notebooks/SPELL_CHECKER_EN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4b_2KemgDWf"
      },
      "source": [
        "\n",
        "\n",
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/SPELL_CHECKER_EN.ipynb)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnZG0I4ogNLI"
      },
      "source": [
        "# **Spell check your text documents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apjCmRyjgQll"
      },
      "source": [
        "## 1. Colab Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2phEj9SygX4n"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uAiXj3DOfyZ-"
      },
      "outputs": [],
      "source": [
        "# Install PySpark and Spark NLP\n",
        "! pip install -q pyspark spark-nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: find spark nlp version\n",
        "\n",
        "!pip show spark-nlp"
      ],
      "metadata": {
        "id": "YvvCMUOQ35pE",
        "outputId": "af1a3da4-56c0-402f-8f42-b1615e8970e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: spark-nlp\n",
            "Version: 6.0.5\n",
            "Summary: John Snow Labs Spark NLP is a natural language processing library built on top of Apache Spark ML. It provides simple, performant & accurate NLP annotations for machine learning pipelines, that scale easily in a distributed environment.\n",
            "Home-page: https://github.com/JohnSnowLabs/spark-nlp\n",
            "Author: John Snow Labs\n",
            "Author-email: \n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: \n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "\n",
        "def initialize_spark_nlp():\n",
        "    try:\n",
        "        spark = sparknlp.start()\n",
        "        print(\"Spark NLP version:\", sparknlp.version())\n",
        "        return spark\n",
        "    except Exception as e:\n",
        "        print(\"Error initializing Spark NLP session:\", str(e))\n",
        "        raise\n",
        "\n",
        "def load_pipeline(pipeline_name='check_spelling', lang='en'):\n",
        "\n",
        "    try:\n",
        "        return PretrainedPipeline(pipeline_name, lang=lang)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading pipeline '{pipeline_name}':\", str(e))\n",
        "        raise\n",
        "\n",
        "def get_corrected_text(annotations):\n",
        "    try:\n",
        "        corrected_tokens = [token.result for token in annotations['checked']]\n",
        "        return \" \".join(corrected_tokens).replace(\" ,\", \",\").replace(\" .\", \".\")\n",
        "    except KeyError:\n",
        "        print(\"Error: 'checked' key not found in annotations.\")\n",
        "        return \"\"\n",
        "\n",
        "def main():\n",
        "    text = (\n",
        "        \"Yesturday, I went to the libary to borow a book about anciant civilizations. \"\n",
        "        \"The wether was pleasent, so I decidid to walk insted of taking the buss. On the way, \"\n",
        "        \"I saw a restuarent that lookt intresting, and I plan to viset it soon.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Initialize Spark NLP and load the pipeline\n",
        "        spark = initialize_spark_nlp()\n",
        "        pipeline = load_pipeline()\n",
        "\n",
        "        # Annotate text\n",
        "        annotations = pipeline.fullAnnotate(text)[0]\n",
        "\n",
        "        # Get and print corrected text\n",
        "        corrected_text = get_corrected_text(annotations)\n",
        "        print(\"*\"*77)\n",
        "        print(\"Original Text:\\n\", text)\n",
        "        print(\"Corrected Text:\\n\", corrected_text)\n",
        "        print(\"*\"*77)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An unexpected error occurred:\", str(e))\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DJnasw_YhUo",
        "outputId": "a25430a2-169b-4725-cbb5-c0e46d61d488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning::Spark Session already created, some configs may not take.\n",
            "Spark NLP version: 6.0.5\n",
            "check_spelling download started this may take some time.\n",
            "Approx size to download 884.9 KB\n",
            "[OK!]\n",
            "*****************************************************************************\n",
            "Original Text:\n",
            " Yesturday, I went to the libary to borow a book about anciant civilizations. The wether was pleasent, so I decidid to walk insted of taking the buss. On the way, I saw a restuarent that lookt intresting, and I plan to viset it soon.\n",
            "Corrected Text:\n",
            " Yesterday, I went to the library to borrow a book about ancient civilizations. The whether was pleasant, so I decided to walk instead of taking the bus. On the way, I saw a restuarent that looks interesting, and I plan to visit it soon.\n",
            "*****************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-6.0.5.jar -O spark-nlp-6.0.5.jar\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPk_t4TfhCcs",
        "outputId": "ba25b14d-45dc-4485-b244-527182d6571f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-15 13:20:29--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-6.0.5.jar\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 3.5.23.15, 54.231.224.192, 16.15.176.114, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|3.5.23.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 656279608 (626M) [application/java-archive]\n",
            "Saving to: ‘spark-nlp-6.0.5.jar’\n",
            "\n",
            "spark-nlp-6.0.5.jar 100%[===================>] 625.88M  58.3MB/s    in 12s     \n",
            "\n",
            "2025-07-15 13:20:40 (53.6 MB/s) - ‘spark-nlp-6.0.5.jar’ saved [656279608/656279608]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.path.exists(\"/content/spark-nlp-6.0.5.jar\")\n"
      ],
      "metadata": {
        "id": "QIDYSpBC4PIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5845295-7a71-4b6b-8717-debfb643dd86"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "from pyspark.sql import SparkSession\n",
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "def initialize_spark_nlp():\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"check_spelling\") \\\n",
        "        .config(\"spark.jars\", \"/content/spark-nlp-6.0.5.jar\") \\\n",
        "        .getOrCreate()\n",
        "    return spark\n",
        "\n",
        "def load_pipeline(pipeline_name='check_spelling', lang='en'):\n",
        "    try:\n",
        "        return PretrainedPipeline(pipeline_name, lang=lang)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading pipeline '{pipeline_name}':\", str(e))\n",
        "        raise\n",
        "\n",
        "def get_corrected_text(annotations):\n",
        "    try:\n",
        "        corrected_tokens = [token.result for token in annotations['checked']]\n",
        "        return \" \".join(corrected_tokens).replace(\" ,\", \",\").replace(\" .\", \".\")\n",
        "    except KeyError:\n",
        "        print(\"Error: 'checked' key not found in annotations.\")\n",
        "        return \"\"\n",
        "\n",
        "def main():\n",
        "    text = (\n",
        "        \"Yesturday, I went to the libary to borow a book about anciant civilizations. \"\n",
        "        \"The wether was pleasent, so I decidid to walk insted of taking the buss. On the way, \"\n",
        "        \"I saw a restuarent that lookt intresting, and I plan to viset it soon.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Initialize Spark NLP and load the pipeline\n",
        "        spark = initialize_spark_nlp()\n",
        "        pipeline = load_pipeline()\n",
        "\n",
        "        # Annotate text\n",
        "        annotations = pipeline.fullAnnotate(text)[0]\n",
        "\n",
        "        # Get and print corrected text\n",
        "        corrected_text = get_corrected_text(annotations)\n",
        "        print(\"*\"*77)\n",
        "        print(\"Original Text:\\n\", text)\n",
        "        print(\"Corrected Text:\\n\", corrected_text)\n",
        "        print(\"*\"*77)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An unexpected error occurred:\", str(e))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5r5-n3LgywO",
        "outputId": "8ec87b25-a4da-4b54-dae8-b68e896680aa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check_spelling download started this may take some time.\n",
            "Error loading pipeline 'check_spelling': 'JavaPackage' object is not callable\n",
            "An unexpected error occurred: 'JavaPackage' object is not callable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import sparknlp\n",
        "from pyspark.sql import SparkSession\n",
        "from sparknlp.base import DocumentAssembler, Finisher\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "\n",
        "# Initialize Spark NLP\n",
        "\n",
        "def initialize_spark_nlp():\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"spellcheck_models\") \\\n",
        "        .config(\"spark.jars\", \"/content/spark-nlp-6.0.5.jar\") \\\n",
        "        .config(\"spark.driver.memory\", \"8g\") \\\n",
        "        .config(\"spark.executor.memory\", \"8g\") \\\n",
        "        .getOrCreate()\n",
        "    return spark\n",
        "\n",
        "# Global Spark session and DocumentAssembler\n",
        "spark = initialize_spark_nlp()\n",
        "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
        "finisher = Finisher().setInputCols([\"spell\"])\n",
        "\n",
        "# Load and define each spell check model pipeline\n",
        "\n",
        "def load_spellcheck_dl():\n",
        "    spell_model = ContextSpellCheckerModel.pretrained(\"spellcheck_dl\", lang=\"en\") \\\n",
        "        .setInputCols([\"token\"]).setOutputCol(\"spell\")\n",
        "\n",
        "    pipeline = Pipeline(stages=[document_assembler, tokenizer, spell_model, finisher])\n",
        "    return pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))\n",
        "\n",
        "def load_spellcheck_norvig():\n",
        "    spell_model = NorvigSweetingModel.pretrained(\"spellcheck_norvig\", lang=\"en\") \\\n",
        "        .setInputCols([\"token\"]).setOutputCol(\"spell\")\n",
        "\n",
        "    pipeline = Pipeline(stages=[document_assembler, tokenizer, spell_model, finisher])\n",
        "    return pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))\n",
        "\n",
        "def load_spellcheck_sd():\n",
        "    spell_model = SymmetricDeleteModel.pretrained(\"spellcheck_sd\", lang=\"en\") \\\n",
        "        .setInputCols([\"token\"]).setOutputCol(\"spell\")\n",
        "\n",
        "    pipeline = Pipeline(stages=[document_assembler, tokenizer, spell_model, finisher])\n",
        "    return pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))\n",
        "\n",
        "# Helper function to correct and return text\n",
        "\n",
        "def correct_text(pipeline_model, input_text):\n",
        "    try:\n",
        "        df = spark.createDataFrame([[input_text]]).toDF(\"text\")\n",
        "        result = pipeline_model.transform(df)\n",
        "        corrected = result.select(\"finished_spell\").first()[0]\n",
        "        return \" \".join(corrected)\n",
        "    except Exception as e:\n",
        "        print(\"Error during correction:\", str(e))\n",
        "        return \"\"\n",
        "\n",
        "# Sample usage\n",
        "\n",
        "def demo_model(model_name):\n",
        "    sample_text = '''Yesturday, I went to the libary to borow a book about anciant civilizations.\n",
        "    The wether was pleasent, so I decidid to walk insted of taking the buss. On the way,\n",
        "    I saw a restuarent that lookt intresting, and I plan to viset it soon. I lke aple. we needto separate the words whereit is needed.'''\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Running spell check using: {model_name}\")\n",
        "    print(\"Original:\", sample_text)\n",
        "\n",
        "    if model_name == \"spellcheck_dl\":\n",
        "        model = load_spellcheck_dl()\n",
        "    elif model_name == \"spellcheck_norvig\":\n",
        "        model = load_spellcheck_norvig()\n",
        "    elif model_name == \"spellcheck_sd\":\n",
        "        model = load_spellcheck_sd()\n",
        "    else:\n",
        "        print(\"Invalid model name\")\n",
        "        return\n",
        "\n",
        "    corrected = correct_text(model, sample_text)\n",
        "    print(\"Corrected:\", corrected)\n",
        "    print(\"=\"*70)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo_model(\"spellcheck_dl\")\n",
        "    demo_model(\"spellcheck_norvig\")\n",
        "    # demo_model(\"spellcheck_sd\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "collapsed": true,
        "id": "QTrUdmJippqO",
        "outputId": "4eb8804f-73c7-4091-d784-83ec59bf8cec"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'JavaPackage' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-28-3699156375.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Global Spark session and DocumentAssembler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_spark_nlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdocument_assembler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocumentAssembler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"document\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInputCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"document\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"token\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mfinisher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFinisher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInputCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"spell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_F\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sparknlp/base/document_assembler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mkeyword_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDocumentAssembler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"com.johnsnowlabs.nlp.DocumentAssembler\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"document\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleanupMode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'disabled'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_F\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sparknlp/internal/annotator_transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, classname)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_class_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mjava_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjava_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk -y\n",
        "!update-alternatives --set java /usr/lib/jvm/java-11-openjdk-amd64/bin/java\n",
        "!java -version\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eBJ_h3pnQM48",
        "outputId": "6cc74cca-d5fa-4454-f275-d460a5ac9d04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxtst6 libxxf86dga1 openjdk-11-jre\n",
            "  x11-utils\n",
            "Suggested packages:\n",
            "  libxt-doc openjdk-11-demo openjdk-11-source visualvm mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxtst6 libxxf86dga1 openjdk-11-jdk\n",
            "  openjdk-11-jre x11-utils\n",
            "0 upgraded, 10 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 6,920 kB of archives.\n",
            "After this operation, 16.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.27+6~us1-0ubuntu1~22.04 [214 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk amd64 11.0.27+6~us1-0ubuntu1~22.04 [2,895 kB]\n",
            "Fetched 6,920 kB in 0s (14.8 MB/s)\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "(Reading database ... 126281 files and directories currently installed.)\n",
            "Preparing to unpack .../0-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../1-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../2-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../3-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../4-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../5-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../6-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../7-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../8-openjdk-11-jre_11.0.27+6~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-11-jdk:amd64.\n",
            "Preparing to unpack .../9-openjdk-11-jdk_11.0.27+6~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk:amd64 (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up openjdk-11-jdk:amd64 (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "openjdk version \"11.0.27\" 2025-04-15\n",
            "OpenJDK Runtime Environment (build 11.0.27+6-post-Ubuntu-0ubuntu122.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.27+6-post-Ubuntu-0ubuntu122.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.4.1\n",
        "!pip install spark-nlp==6.0.5\n"
      ],
      "metadata": {
        "id": "MveUpOj1QOhK",
        "outputId": "6587c5fc-06ce-442b-d78a-2874f50ce1cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.4.1\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark==3.4.1) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285391 sha256=33229b2afa1c06a652a22b0664a8c17247922340b395fa98208561c5006c827b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/b4/d8/38accc42606f6675165423e9f0236f8e825f6b6b6048d6743e\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.1\n",
            "    Uninstalling pyspark-3.5.1:\n",
            "      Successfully uninstalled pyspark-3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.8.2 requires pyspark[connect]~=3.5.1, but you have pyspark 3.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyspark-3.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyspark"
                ]
              },
              "id": "b355badf7c364b1b9f34d7531fd47fe8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spark-nlp==6.0.5 in /usr/local/lib/python3.11/dist-packages (6.0.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip show pyspark"
      ],
      "metadata": {
        "id": "vuwGKxpdQTnp",
        "outputId": "9b315eec-441c-4394-ec83-e0e4b9bf9722",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: pyspark\n",
            "Version: 3.4.1\n",
            "Summary: Apache Spark Python API\n",
            "Home-page: https://github.com/apache/spark/tree/master/python\n",
            "Author: Spark Developers\n",
            "Author-email: dev@spark.apache.org\n",
            "License: http://www.apache.org/licenses/LICENSE-2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: py4j\n",
            "Required-by: dataproc-spark-connect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip show spark-nlp"
      ],
      "metadata": {
        "id": "jULMLnBmQWFm",
        "outputId": "044b93ff-e4ec-4692-ce00-f5d1358b0339",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: spark-nlp\n",
            "Version: 6.0.5\n",
            "Summary: John Snow Labs Spark NLP is a natural language processing library built on top of Apache Spark ML. It provides simple, performant & accurate NLP annotations for machine learning pipelines, that scale easily in a distributed environment.\n",
            "Home-page: https://github.com/JohnSnowLabs/spark-nlp\n",
            "Author: John Snow Labs\n",
            "Author-email: \n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: \n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-6.0.5.jar -O spark-nlp-6.0.5.jar\n"
      ],
      "metadata": {
        "id": "QKrIkY2AQf_L",
        "outputId": "b5ffff5e-0325-49c9-a39a-5ec98ddf4c66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-15 13:25:33--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-6.0.5.jar\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.124.184, 3.5.22.52, 52.216.34.144, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.124.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 656279608 (626M) [application/java-archive]\n",
            "Saving to: ‘spark-nlp-6.0.5.jar’\n",
            "\n",
            "spark-nlp-6.0.5.jar 100%[===================>] 625.88M  42.1MB/s    in 11s     \n",
            "\n",
            "2025-07-15 13:25:44 (58.5 MB/s) - ‘spark-nlp-6.0.5.jar’ saved [656279608/656279608]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "from pyspark.sql import SparkSession\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "\n",
        "def initialize_spark_nlp():\n",
        "    return SparkSession.builder \\\n",
        "        .appName(\"Spark NLP\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.driver.memory\", \"4G\") \\\n",
        "        .config(\"spark.jars\", \"/content/spark-nlp-6.0.5.jar\") \\\n",
        "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "        .config(\"spark.kryoserializer.buffer.max\", \"1000M\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark = initialize_spark_nlp()\n",
        "\n",
        "# Step 3: Spell check pipeline runner\n",
        "def try_pretrained_spellcheck_pipeline(pipeline_name, text):\n",
        "    try:\n",
        "        print(\"\\n\" + \"=\" * 90)\n",
        "        print(f\"Running pipeline: {pipeline_name}\")\n",
        "        print(\"Original Text:\\n\", text)\n",
        "\n",
        "        pipeline = PretrainedPipeline(pipeline_name, lang=\"en\")\n",
        "        result = pipeline.annotate(text)\n",
        "\n",
        "        # Dynamic output key resolution\n",
        "        if \"checked\" in result:\n",
        "            corrected_text = result[\"checked\"]\n",
        "        elif \"spell\" in result:\n",
        "            corrected_text = \" \".join(result[\"spell\"])\n",
        "        elif \"finished_spell\" in result:\n",
        "            corrected_text = \" \".join(result[\"finished_spell\"])\n",
        "        else:\n",
        "            corrected_text = \"[No corrected output found]\"\n",
        "\n",
        "        print(\"\\nCorrected Output:\\n\", corrected_text)\n",
        "        print(\"=\" * 90)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Pipeline {pipeline_name} failed. Reason: {e}\")\n",
        "\n",
        "# Step 2: Sample text with spelling errors\n",
        "sample_text = '''Yesturday, I went to the libary to borow a book about anciant civilizations.\n",
        "The wether was pleasent, so I decidid to walk insted of taking the buss. On the way,\n",
        "I saw a restuarent that lookt intresting, and I plan to viset it soon. I lke aple. we needto separate the words whereit is needed.'''\n",
        "\n",
        "pipelines = [\n",
        "        \"check_spelling\",\n",
        "        # \"check_spelling_dl\"\n",
        "        'spellcheck_dl_pipeline'\n",
        "    ]\n",
        "\n",
        "for pipeline_name in pipelines:\n",
        "  try_pretrained_spellcheck_pipeline(pipeline_name, sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGXJ7Ui41Rli",
        "outputId": "6c98a6b1-511a-4501-b66d-92febddd1d7b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========================================================================================\n",
            "Running pipeline: check_spelling\n",
            "Original Text:\n",
            " Yesturday, I went to the libary to borow a book about anciant civilizations.\n",
            "The wether was pleasent, so I decidid to walk insted of taking the buss. On the way,\n",
            "I saw a restuarent that lookt intresting, and I plan to viset it soon. I lke aple. we needto separate the words whereit is needed.\n",
            "check_spelling download started this may take some time.\n",
            "Approx size to download 884.9 KB\n",
            "[OK!]\n",
            "\n",
            "Corrected Output:\n",
            " ['Yesterday', ',', 'I', 'went', 'to', 'the', 'library', 'to', 'borrow', 'a', 'book', 'about', 'ancient', 'civilizations', '.', 'The', 'whether', 'was', 'pleasant', ',', 'so', 'I', 'decided', 'to', 'walk', 'instead', 'of', 'taking', 'the', 'bus', '.', 'On', 'the', 'way', ',', 'I', 'saw', 'a', 'restuarent', 'that', 'looks', 'interesting', ',', 'and', 'I', 'plan', 'to', 'visit', 'it', 'soon', '.', 'I', 'lke', 'able', '.', 'we', 'needto', 'separate', 'the', 'words', 'wherein', 'is', 'needed', '.']\n",
            "==========================================================================================\n",
            "\n",
            "==========================================================================================\n",
            "Running pipeline: spellcheck_dl_pipeline\n",
            "Original Text:\n",
            " Yesturday, I went to the libary to borow a book about anciant civilizations.\n",
            "The wether was pleasent, so I decidid to walk insted of taking the buss. On the way,\n",
            "I saw a restuarent that lookt intresting, and I plan to viset it soon. I lke aple. we needto separate the words whereit is needed.\n",
            "spellcheck_dl_pipeline download started this may take some time.\n",
            "Approx size to download 95.1 MB\n",
            "[OK!]\n",
            "\n",
            "Corrected Output:\n",
            " ['Yesterday', ',', 'I', 'went', 'to', 'the', 'library', 'to', 'borrow', 'a', 'book', 'about', 'ancient', 'civilizations', '.', 'The', 'weather', 'was', 'pleasant', ',', 'so', 'I', 'decided', 'to', 'walk', 'instead', 'of', 'taking', 'the', 'bus', '.', 'On', 'the', 'way', ',', 'I', 'saw', 'a', 'restaurant', 'that', 'look', 'interesting', ',', 'and', 'I', 'plan', 'to', 'visit', 'it', 'soon', '.', 'I', 'like', 'able', '.', 'we', 'need', 'separate', 'the', 'words', 'where', 'is', 'needed', '.']\n",
            "==========================================================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "45150093197569bb3a58481dcd32cd1adb45462fa3448719e8ac38ada6166aca"
    },
    "kernelspec": {
      "display_name": "Python 3.6.10 64-bit ('tensorflow2_p36': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}