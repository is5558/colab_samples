{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/is5558/colab_samples/blob/main/tutorials/streamlit_notebooks/SPELL_CHECKER_EN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4b_2KemgDWf"
      },
      "source": [
        "\n",
        "\n",
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/SPELL_CHECKER_EN.ipynb)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnZG0I4ogNLI"
      },
      "source": [
        "# **Spell check your text documents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apjCmRyjgQll"
      },
      "source": [
        "## 1. Colab Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2phEj9SygX4n"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uAiXj3DOfyZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cec61576-98b1-4894-d8c3-5746baea170d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/718.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/718.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m716.8/718.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.9/718.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install PySpark and Spark NLP\n",
        "! pip install -q pyspark spark-nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "\n",
        "def initialize_spark_nlp():\n",
        "    try:\n",
        "        spark = sparknlp.start()\n",
        "        print(\"Spark NLP version:\", sparknlp.version())\n",
        "        return spark\n",
        "    except Exception as e:\n",
        "        print(\"Error initializing Spark NLP session:\", str(e))\n",
        "        raise\n",
        "\n",
        "def load_pipeline(pipeline_name='check_spelling', lang='en'):\n",
        "\n",
        "    try:\n",
        "        return PretrainedPipeline(pipeline_name, lang=lang)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading pipeline '{pipeline_name}':\", str(e))\n",
        "        raise\n",
        "\n",
        "def get_corrected_text(annotations):\n",
        "    try:\n",
        "        corrected_tokens = [token.result for token in annotations['checked']]\n",
        "        return \" \".join(corrected_tokens).replace(\" ,\", \",\").replace(\" .\", \".\")\n",
        "    except KeyError:\n",
        "        print(\"Error: 'checked' key not found in annotations.\")\n",
        "        return \"\"\n",
        "\n",
        "def main():\n",
        "    text = (\n",
        "        \"Yesturday, I went to the libary to borow a book about anciant civilizations. \"\n",
        "        \"The wether was pleasent, so I decidid to walk insted of taking the buss. On the way, \"\n",
        "        \"I saw a restuarent that lookt intresting, and I plan to viset it soon.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Initialize Spark NLP and load the pipeline\n",
        "        spark = initialize_spark_nlp()\n",
        "        pipeline = load_pipeline()\n",
        "\n",
        "        # Annotate text\n",
        "        annotations = pipeline.fullAnnotate(text)[0]\n",
        "\n",
        "        # Get and print corrected text\n",
        "        corrected_text = get_corrected_text(annotations)\n",
        "        print(\"*\"*77)\n",
        "        print(\"Original Text:\\n\", text)\n",
        "        print(\"Corrected Text:\\n\", corrected_text)\n",
        "        print(\"*\"*77)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An unexpected error occurred:\", str(e))\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DJnasw_YhUo",
        "outputId": "a25430a2-169b-4725-cbb5-c0e46d61d488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning::Spark Session already created, some configs may not take.\n",
            "Spark NLP version: 6.0.5\n",
            "check_spelling download started this may take some time.\n",
            "Approx size to download 884.9 KB\n",
            "[OK!]\n",
            "*****************************************************************************\n",
            "Original Text:\n",
            " Yesturday, I went to the libary to borow a book about anciant civilizations. The wether was pleasent, so I decidid to walk insted of taking the buss. On the way, I saw a restuarent that lookt intresting, and I plan to viset it soon.\n",
            "Corrected Text:\n",
            " Yesterday, I went to the library to borrow a book about ancient civilizations. The whether was pleasant, so I decided to walk instead of taking the bus. On the way, I saw a restuarent that looks interesting, and I plan to visit it soon.\n",
            "*****************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-6.0.5.jar -O spark-nlp-6.0.5.jar\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPk_t4TfhCcs",
        "outputId": "a87598fd-6360-456c-d991-88faa36c49b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-15 10:39:56--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-6.0.5.jar\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.235.24, 16.182.102.208, 3.5.24.115, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.235.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 656279608 (626M) [application/java-archive]\n",
            "Saving to: ‘spark-nlp-6.0.5.jar’\n",
            "\n",
            "spark-nlp-6.0.5.jar 100%[===================>] 625.88M  74.7MB/s    in 12s     \n",
            "\n",
            "2025-07-15 10:40:08 (53.2 MB/s) - ‘spark-nlp-6.0.5.jar’ saved [656279608/656279608]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "from pyspark.sql import SparkSession\n",
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "def initialize_spark_nlp():\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"check_spelling\") \\\n",
        "        .config(\"spark.jars\", \"/content/spark-nlp-6.0.5.jar\") \\\n",
        "        .getOrCreate()\n",
        "    return spark\n",
        "\n",
        "def load_pipeline(pipeline_name='check_spelling', lang='en'):\n",
        "    try:\n",
        "        return PretrainedPipeline(pipeline_name, lang=lang)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading pipeline '{pipeline_name}':\", str(e))\n",
        "        raise\n",
        "\n",
        "def get_corrected_text(annotations):\n",
        "    try:\n",
        "        corrected_tokens = [token.result for token in annotations['checked']]\n",
        "        return \" \".join(corrected_tokens).replace(\" ,\", \",\").replace(\" .\", \".\")\n",
        "    except KeyError:\n",
        "        print(\"Error: 'checked' key not found in annotations.\")\n",
        "        return \"\"\n",
        "\n",
        "def main():\n",
        "    text = (\n",
        "        \"Yesturday, I went to the libary to borow a book about anciant civilizations. \"\n",
        "        \"The wether was pleasent, so I decidid to walk insted of taking the buss. On the way, \"\n",
        "        \"I saw a restuarent that lookt intresting, and I plan to viset it soon.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Initialize Spark NLP and load the pipeline\n",
        "        spark = initialize_spark_nlp()\n",
        "        pipeline = load_pipeline()\n",
        "\n",
        "        # Annotate text\n",
        "        annotations = pipeline.fullAnnotate(text)[0]\n",
        "\n",
        "        # Get and print corrected text\n",
        "        corrected_text = get_corrected_text(annotations)\n",
        "        print(\"*\"*77)\n",
        "        print(\"Original Text:\\n\", text)\n",
        "        print(\"Corrected Text:\\n\", corrected_text)\n",
        "        print(\"*\"*77)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An unexpected error occurred:\", str(e))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5r5-n3LgywO",
        "outputId": "49e955c1-3850-481d-e466-5054f4488a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check_spelling download started this may take some time.\n",
            "Approx size to download 884.9 KB\n",
            "[OK!]\n",
            "*****************************************************************************\n",
            "Original Text:\n",
            " Yesturday, I went to the libary to borow a book about anciant civilizations. The wether was pleasent, so I decidid to walk insted of taking the buss. On the way, I saw a restuarent that lookt intresting, and I plan to viset it soon.\n",
            "Corrected Text:\n",
            " Yesterday, I went to the library to borrow a book about ancient civilizations. The whether was pleasant, so I decided to walk instead of taking the bus. On the way, I saw a restuarent that looks interesting, and I plan to visit it soon.\n",
            "*****************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Initialize Spark NLP\n",
        "\n",
        "def initialize_spark_nlp():\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"spellcheck_models\") \\\n",
        "        .config(\"spark.jars\", \"/content/spark-nlp-6.0.5.jar\") \\\n",
        "        .config(\"spark.driver.memory\", \"8g\") \\\n",
        "        .config(\"spark.executor.memory\", \"8g\") \\\n",
        "        .getOrCreate()\n",
        "    return spark\n",
        "\n",
        "# Global Spark session and DocumentAssembler\n",
        "spark = initialize_spark_nlp()\n",
        "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
        "finisher = Finisher().setInputCols([\"spell\"])\n",
        "\n",
        "# Load and define each spell check model pipeline\n",
        "\n",
        "def load_spellcheck_dl():\n",
        "    spell_model = ContextSpellCheckerModel.pretrained(\"spellcheck_dl\", lang=\"en\") \\\n",
        "        .setInputCols([\"token\"]).setOutputCol(\"spell\")\n",
        "\n",
        "    pipeline = Pipeline(stages=[document_assembler, tokenizer, spell_model, finisher])\n",
        "    return pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))\n",
        "\n",
        "def load_spellcheck_norvig():\n",
        "    spell_model = NorvigSweetingModel.pretrained(\"spellcheck_norvig\", lang=\"en\") \\\n",
        "        .setInputCols([\"token\"]).setOutputCol(\"spell\")\n",
        "\n",
        "    pipeline = Pipeline(stages=[document_assembler, tokenizer, spell_model, finisher])\n",
        "    return pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))\n",
        "\n",
        "def load_spellcheck_sd():\n",
        "    spell_model = SymmetricDeleteModel.pretrained(\"spellcheck_sd\", lang=\"en\") \\\n",
        "        .setInputCols([\"token\"]).setOutputCol(\"spell\")\n",
        "\n",
        "    pipeline = Pipeline(stages=[document_assembler, tokenizer, spell_model, finisher])\n",
        "    return pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))\n",
        "\n",
        "# Helper function to correct and return text\n",
        "\n",
        "def correct_text(pipeline_model, input_text):\n",
        "    try:\n",
        "        df = spark.createDataFrame([[input_text]]).toDF(\"text\")\n",
        "        result = pipeline_model.transform(df)\n",
        "        corrected = result.select(\"finished_spell\").first()[0]\n",
        "        return \" \".join(corrected)\n",
        "    except Exception as e:\n",
        "        print(\"Error during correction:\", str(e))\n",
        "        return \"\"\n",
        "\n",
        "# Sample usage\n",
        "\n",
        "def demo_model(model_name):\n",
        "    sample_text = '''Yesturday, I went to the libary to borow a book about anciant civilizations.\n",
        "    The wether was pleasent, so I decidid to walk insted of taking the buss. On the way,\n",
        "    I saw a restuarent that lookt intresting, and I plan to viset it soon. I lke aple. we needto separate the words whereit is needed.'''\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Running spell check using: {model_name}\")\n",
        "    print(\"Original:\", sample_text)\n",
        "\n",
        "    if model_name == \"spellcheck_dl\":\n",
        "        model = load_spellcheck_dl()\n",
        "    elif model_name == \"spellcheck_norvig\":\n",
        "        model = load_spellcheck_norvig()\n",
        "    elif model_name == \"spellcheck_sd\":\n",
        "        model = load_spellcheck_sd()\n",
        "    else:\n",
        "        print(\"Invalid model name\")\n",
        "        return\n",
        "\n",
        "    corrected = correct_text(model, sample_text)\n",
        "    print(\"Corrected:\", corrected)\n",
        "    print(\"=\"*70)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo_model(\"spellcheck_dl\")\n",
        "    demo_model(\"spellcheck_norvig\")\n",
        "    # demo_model(\"spellcheck_sd\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "collapsed": true,
        "id": "QTrUdmJippqO",
        "outputId": "f10dfaeb-c965-4289-8e51-d95f4072137a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Running spell check using: spellcheck_sd\n",
            "Original: Yesturday, I went to the libary to borow a book about anciant civilizations.\n",
            "    The wether was pleasent, so I decidid to walk insted of taking the buss. On the way,\n",
            "    I saw a restuarent that lookt intresting, and I plan to viset it soon. I lke aple. we needto separate the words whereit is needed.\n",
            "spellcheck_sd download started this may take some time.\n",
            "Approximate size to download 198.1 MB\n",
            "[ / ]\n",
            "An error occurred while calling z:com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadModel.\n",
            ": java.lang.OutOfMemoryError: Java heap space\n",
            "[OK!]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadModel.\n: java.lang.OutOfMemoryError: Java heap space\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-457505869.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mdemo_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spellcheck_sd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-6-457505869.py\u001b[0m in \u001b[0;36mdemo_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"spellcheck_sd\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_spellcheck_sd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;31m# elif model_name == \"spellcheck_norvig\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m#     model = load_spellcheck_norvig()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-6-457505869.py\u001b[0m in \u001b[0;36mload_spellcheck_sd\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_spellcheck_sd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mspell_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSymmetricDeleteModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spellcheck_sd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0msetInputCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spell\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sparknlp/annotator/spell_check/symmetric_delete.py\u001b[0m in \u001b[0;36mpretrained\u001b[0;34m(name, lang, remote_loc)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \"\"\"\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0msparknlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResourceDownloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mResourceDownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSymmetricDeleteModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremote_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sparknlp/pretrained/resource_downloader.py\u001b[0m in \u001b[0;36mdownloadModel\u001b[0;34m(reader, name, language, remote_loc, j_dwn)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mstop_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sparknlp/pretrained/resource_downloader.py\u001b[0m in \u001b[0;36mdownloadModel\u001b[0;34m(reader, name, language, remote_loc, j_dwn)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mj_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_DownloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremote_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj_dwn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sparknlp/internal/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, reader, name, language, remote_loc, validator)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_DownloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExtendedJavaWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremote_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         super(_DownloadModel, self).__init__(\n\u001b[0m\u001b[1;32m    718\u001b[0m             \u001b[0;34m\"com.johnsnowlabs.nlp.pretrained.\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalidator\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".downloadModel\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sparknlp/internal/extended_java_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, java_obj, *args)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExtendedJavaWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sparknlp/internal/extended_java_wrapper.py\u001b[0m in \u001b[0;36mnew_java_obj\u001b[0;34m(self, java_class, *args)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_java_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_java_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpylist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mjava_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjava_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadModel.\n: java.lang.OutOfMemoryError: Java heap space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "from pyspark.sql import SparkSession\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "\n",
        "# Step 1: Initialize Spark NLP Session\n",
        "def initialize_spark_nlp():\n",
        "    return SparkSession.builder \\\n",
        "        .appName(\"SpellCheck_Pipelines_6.0\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.driver.memory\", \"8G\") \\\n",
        "        .config(\"spark.executor.memory\", \"8G\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark = initialize_spark_nlp()\n",
        "\n",
        "# Step 2: Sample text with spelling errors\n",
        "sample_text = '''Yesturday, I went to the libary to borow a book about anciant civilizations.\n",
        "The wether was pleasent, so I decidid to walk insted of taking the buss. On the way,\n",
        "I saw a restuarent that lookt intresting, and I plan to viset it soon. I lke aple. we needto separate the words whereit is needed.'''\n",
        "\n",
        "# Step 3: Spell check pipeline runner\n",
        "def try_pretrained_spellcheck_pipeline(pipeline_name, text):\n",
        "    try:\n",
        "        print(\"\\n\" + \"=\" * 90)\n",
        "        print(f\"Running pipeline: {pipeline_name}\")\n",
        "        print(\"Original Text:\\n\", text)\n",
        "\n",
        "        pipeline = PretrainedPipeline(pipeline_name, lang=\"en\")\n",
        "        result = pipeline.annotate(text)\n",
        "\n",
        "        # Dynamic output key resolution\n",
        "        if \"checked\" in result:\n",
        "            corrected_text = result[\"checked\"]\n",
        "        elif \"spell\" in result:\n",
        "            corrected_text = \" \".join(result[\"spell\"])\n",
        "        elif \"finished_spell\" in result:\n",
        "            corrected_text = \" \".join(result[\"finished_spell\"])\n",
        "        else:\n",
        "            corrected_text = \"[No corrected output found]\"\n",
        "\n",
        "        print(\"\\nCorrected Output:\\n\", corrected_text)\n",
        "        print(\"=\" * 90)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Pipeline {pipeline_name} failed. Reason: {e}\")\n",
        "\n",
        "# Step 4: Run all verified pipelines\n",
        "if __name__ == \"__main__\":\n",
        "    pipelines = [\n",
        "        \"spellcheck_dl_pipeline\",\n",
        "        \"check_spelling\",\n",
        "        \"check_spelling_dl\"\n",
        "    ]\n",
        "\n",
        "    for pipeline_name in pipelines:\n",
        "        try_pretrained_spellcheck_pipeline(pipeline_name, sample_text)\n"
      ],
      "metadata": {
        "id": "sGXJ7Ui41Rli",
        "outputId": "0ceaedc4-b194-49b1-b4c6-37b885833901",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========================================================================================\n",
            "Running pipeline: spellcheck_dl_pipeline\n",
            "Original Text:\n",
            " Yesturday, I went to the libary to borow a book about anciant civilizations.\n",
            "The wether was pleasent, so I decidid to walk insted of taking the buss. On the way,\n",
            "I saw a restuarent that lookt intresting, and I plan to viset it soon. I lke aple. we needto separate the words whereit is needed.\n",
            "spellcheck_dl_pipeline download started this may take some time.\n",
            "Approx size to download 95.1 MB\n",
            "[OK!]\n",
            "\n",
            "Corrected Output:\n",
            " ['Yesterday', ',', 'I', 'went', 'to', 'the', 'library', 'to', 'borrow', 'a', 'book', 'about', 'ancient', 'civilizations', '.', 'The', 'weather', 'was', 'pleasant', ',', 'so', 'I', 'decided', 'to', 'walk', 'instead', 'of', 'taking', 'the', 'bus', '.', 'On', 'the', 'way', ',', 'I', 'saw', 'a', 'restaurant', 'that', 'look', 'interesting', ',', 'and', 'I', 'plan', 'to', 'visit', 'it', 'soon', '.', 'I', 'like', 'able', '.', 'we', 'need', 'separate', 'the', 'words', 'where', 'is', 'needed', '.']\n",
            "==========================================================================================\n",
            "\n",
            "==========================================================================================\n",
            "Running pipeline: spellcheck_sd_pipeline\n",
            "Original Text:\n",
            " Yesturday, I went to the libary to borow a book about anciant civilizations.\n",
            "The wether was pleasent, so I decidid to walk insted of taking the buss. On the way,\n",
            "I saw a restuarent that lookt intresting, and I plan to viset it soon. I lke aple. we needto separate the words whereit is needed.\n",
            "spellcheck_sd_pipeline download started this may take some time.\n",
            "Can not find the model to download please check the name!\n",
            "[ERROR] Pipeline spellcheck_sd_pipeline failed. Reason: 'NoneType' object has no attribute '_to_java'\n",
            "\n",
            "==========================================================================================\n",
            "Running pipeline: spellcheck_norvig_pipeline\n",
            "Original Text:\n",
            " Yesturday, I went to the libary to borow a book about anciant civilizations.\n",
            "The wether was pleasent, so I decidid to walk insted of taking the buss. On the way,\n",
            "I saw a restuarent that lookt intresting, and I plan to viset it soon. I lke aple. we needto separate the words whereit is needed.\n",
            "spellcheck_norvig_pipeline download started this may take some time.\n",
            "Can not find the model to download please check the name!\n",
            "[ERROR] Pipeline spellcheck_norvig_pipeline failed. Reason: 'NoneType' object has no attribute '_to_java'\n",
            "\n",
            "==========================================================================================\n",
            "Running pipeline: check_spelling\n",
            "Original Text:\n",
            " Yesturday, I went to the libary to borow a book about anciant civilizations.\n",
            "The wether was pleasent, so I decidid to walk insted of taking the buss. On the way,\n",
            "I saw a restuarent that lookt intresting, and I plan to viset it soon. I lke aple. we needto separate the words whereit is needed.\n",
            "check_spelling download started this may take some time.\n",
            "Approx size to download 884.9 KB\n",
            "[OK!]\n",
            "\n",
            "Corrected Output:\n",
            " ['Yesterday', ',', 'I', 'went', 'to', 'the', 'library', 'to', 'borrow', 'a', 'book', 'about', 'ancient', 'civilizations', '.', 'The', 'whether', 'was', 'pleasant', ',', 'so', 'I', 'decided', 'to', 'walk', 'instead', 'of', 'taking', 'the', 'bus', '.', 'On', 'the', 'way', ',', 'I', 'saw', 'a', 'restuarent', 'that', 'looks', 'interesting', ',', 'and', 'I', 'plan', 'to', 'visit', 'it', 'soon', '.', 'I', 'lke', 'able', '.', 'we', 'needto', 'separate', 'the', 'words', 'wherein', 'is', 'needed', '.']\n",
            "==========================================================================================\n",
            "\n",
            "==========================================================================================\n",
            "Running pipeline: check_spelling_dl\n",
            "Original Text:\n",
            " Yesturday, I went to the libary to borow a book about anciant civilizations.\n",
            "The wether was pleasent, so I decidid to walk insted of taking the buss. On the way,\n",
            "I saw a restuarent that lookt intresting, and I plan to viset it soon. I lke aple. we needto separate the words whereit is needed.\n",
            "check_spelling_dl download started this may take some time.\n",
            "Approx size to download 112.6 MB\n",
            "[OK!]\n",
            "[ERROR] Pipeline check_spelling_dl failed. Reason: An error occurred while calling z:com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadPipeline.\n",
            ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 64.0 failed 1 times, most recent failure: Lost task 2.0 in stage 64.0 (TID 305) (e42b65b7bdce executor driver): java.io.InvalidClassException: com.johnsnowlabs.nlp.util.regex.RuleFactory; local class incompatible: stream classdesc serialVersionUID = 3820943135112344102, local class serialVersionUID = 7575128234718007790\n",
            "\tat java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:560)\n",
            "\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2020)\n",
            "\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)\n",
            "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2201)\n",
            "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n",
            "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2134)\n",
            "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1675)\n",
            "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)\n",
            "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)\n",
            "\tat org.apache.spark.util.SparkSerDeUtils.deserialize(SparkSerDeUtils.scala:50)\n",
            "\tat org.apache.spark.util.SparkSerDeUtils.deserialize$(SparkSerDeUtils.scala:41)\n",
            "\tat org.apache.spark.util.Utils$.deserialize(Utils.scala:94)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$objectFile$2(SparkContext.scala:1533)\n",
            "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
            "\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n",
            "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
            "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
            "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
            "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
            "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
            "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
            "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
            "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
            "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
            "\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n",
            "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
            "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
            "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n",
            "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
            "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
            "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n",
            "Driver stacktrace:\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
            "\tat scala.Option.foreach(Option.scala:407)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1492)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
            "\tat org.apache.spark.rdd.RDD.take(RDD.scala:1465)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1506)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
            "\tat org.apache.spark.rdd.RDD.first(RDD.scala:1506)\n",
            "\tat com.johnsnowlabs.nlp.serialization.StructFeature.deserializeObject(Feature.scala:182)\n",
            "\tat com.johnsnowlabs.nlp.serialization.Feature.deserialize(Feature.scala:72)\n",
            "\tat com.johnsnowlabs.nlp.FeaturesReader.$anonfun$load$1(ParamsAndFeaturesReadable.scala:34)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:33)\n",
            "\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:24)\n",
            "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$5(Pipeline.scala:277)\n",
            "\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:160)\n",
            "\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:155)\n",
            "\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:42)\n",
            "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$4(Pipeline.scala:277)\n",
            "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
            "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
            "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
            "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
            "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
            "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
            "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
            "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$3(Pipeline.scala:274)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
            "\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:268)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$7(Pipeline.scala:356)\n",
            "\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:160)\n",
            "\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:155)\n",
            "\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:42)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$6(Pipeline.scala:355)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:355)\n",
            "\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:349)\n",
            "\tat com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadPipeline(ResourceDownloader.scala:526)\n",
            "\tat com.johnsnowlabs.nlp.pretrained.ResourceDownloader$.downloadPipeline(ResourceDownloader.scala:520)\n",
            "\tat com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader$.downloadPipeline(ResourceDownloader.scala:749)\n",
            "\tat com.johnsnowlabs.nlp.pretrained.PythonResourceDownloader.downloadPipeline(ResourceDownloader.scala)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: java.io.InvalidClassException: com.johnsnowlabs.nlp.util.regex.RuleFactory; local class incompatible: stream classdesc serialVersionUID = 3820943135112344102, local class serialVersionUID = 7575128234718007790\n",
            "\tat java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:560)\n",
            "\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2020)\n",
            "\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1870)\n",
            "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2201)\n",
            "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)\n",
            "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2134)\n",
            "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1675)\n",
            "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)\n",
            "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)\n",
            "\tat org.apache.spark.util.SparkSerDeUtils.deserialize(SparkSerDeUtils.scala:50)\n",
            "\tat org.apache.spark.util.SparkSerDeUtils.deserialize$(SparkSerDeUtils.scala:41)\n",
            "\tat org.apache.spark.util.Utils$.deserialize(Utils.scala:94)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$objectFile$2(SparkContext.scala:1533)\n",
            "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
            "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
            "\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n",
            "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
            "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
            "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
            "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
            "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
            "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
            "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
            "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
            "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
            "\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\n",
            "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
            "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
            "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n",
            "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
            "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
            "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1492)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\t... 1 more\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "45150093197569bb3a58481dcd32cd1adb45462fa3448719e8ac38ada6166aca"
    },
    "kernelspec": {
      "display_name": "Python 3.6.10 64-bit ('tensorflow2_p36': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}